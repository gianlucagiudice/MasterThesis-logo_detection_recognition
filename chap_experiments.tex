\chapter{Experiments}
\label{chap:experiments}
This chapter presents the experiments designed to evaluate the system described in chapter \autoref{chap:methods}.
The chapter is structured as follows: \autoref{sec:exp-setup} specifies the setup used for the class incremental learning task; \autoref{sec:exp-cil} discusses the experiments related to the CIL model and \autoref{sec:exp-det} those related to the logo detector; \autoref{sec:exp-kd} describes the experiments relative to the KD; finally, \autoref{sec:exp-end2end} presents the results of experiments combining the logo detector with the CIL classifier.

\section{Setup}
\label{sec:exp-setup}
The developed system is tested considering two different CIL setups: in the first setup, the system is tested on a subset of 100 classes out of the total 2993 in the dataset; in the second setup, the model's scaling capabilities are tested considering the entire dataset. Specifically, the CIL configuration is the following:
\begin{enumerate}
    \item \textbf{100 Classes}: 100 classes are extracted from the initial dataset. For the experiments, a distinction is made between the case in which these classes are extracted randomly or are taken the 100 classes with the highest number of images.
    
    Out of these 100 classes, 30 are used for the initial task, then the remaining 70 classes are added 10 at a time through 7 incremental learning iterations.

    \item \textbf{2993 Classes (entire dataset)}: for experiments which consider the entire dataset, the first 1000 classes are used for the initial task, then follows 8 iterations of incremental learning, each adding 250 new classes.
\end{enumerate}

The train, validation and test set are built from the individual classes. For each of these, the instances are divided as follows:
\begin{itemize}
    \item \textbf{Train set}: 70 \%
    \item \textbf{Validation set}: 10 \%
    \item \textbf{Test set}: 20 \%
\end{itemize}


\section{Classifier: CIL model}
\label{sec:exp-cil}
Top-k accuracy is used to evaluate the performance of the CIL model, focusing on cases with $k=1$ and $k=5$.
Using this performance metric, a classification is considered correct if the label is present among the first $k$ predictions to which the model assigns the highest probability. Thus, the accuracy is calculated as the percentage of the correct predictions.

\subsection{100 Classes}
\subsubsection{100 Classes randomly sampled}
For the first experiments, the 100 classes are randomly sampled from the 2993 classes in the dataset, then the classifier is evaluated on the test set according to the CIL setup described in \autoref{sec:exp-setup}. 

As detailed in \autoref{sec:der-algorithm}, the DER algorithm saves some examples of the 'old' classes and reuses these examples during incremental learning iterations. In the following experiments, the memory dedicated to each old class is of 100 samples.

The first group of experiments aims to compare two types of architecture: ResNet-34 and ResNet-50. In addition, the cases where CNNs are pre-trained on ImageNet or not are also considered. For these experiments, the optimizer is SGD and neither regularization of the model via the dropout layer nor data augmentation is used.

The results of the experiments in picture \autoref{fig:exp1} and table \autoref{table:exp1}, reporting the top-1 and top-5 accuracy of the models at varying CIL tasks on the test set, show that pre-trained CNNs perform better, but there is not much difference between ResNet-34 and ResNet-50. For this reason, the architecture chosen for the experiments to follow is ResNet-34, so as to have a slightly smaller network than ResNet-50, and the network is pre-trained on ImageNet.

Other useful insights can be derived from the training history of a task (e.g. Task 7) which reports the top-1 accuracy on the training and validation set. In fact, as we can see from \autoref{fig:exp1-train_val} the accuracy on the training set is much higher than the validation set, which is a clear sign of overfitting of the model.

\begin{figure}[H]
	\centering
	\subfloat[\centering Top-1 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp1-top1.png} }}%
	\subfloat[\centering Top-5 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp1-top5.png} }}%
	\caption{Top-1 and Top-5 accuracy of the models at varying CIL tasks on the test set.}%
	\label{fig:exp1}%
\end{figure}


\begin{figure}[H]
	\centering
	\subfloat[\centering Accuracy on the training set]{{\includegraphics[width=0.49\textwidth]{images/exp/exp1-train.png} }}%
	\subfloat[\centering Accuracy on the validation set]{{\includegraphics[width=0.49\textwidth]{images/exp/exp1-val.png} }}%
	\caption{Comparison of the accuracy at each training epoch at task 7 between the training and validation set.}%
	\label{fig:exp1-train_val}%
\end{figure}


\begin{table}[H]
    \centering
    \centerline{
    \begin{tabular}{c|c|c|c|c}
        \hline
        \textbf{Name} &
        \textbf{Backbone} &
        \textbf{Pre-trained} &
        \textbf{Top-1} & 
        \textbf{Top-5} \\
        &
        &
        &
        \textbf{acc. (\%)} & 
        \textbf{acc. (\%)} \\
        \hline
        \hline
CIL\_30\_10\_100-mem100-resnet34-SGD-nopretrained-drop0 &ResNet-34&no& 52.97 & 87.02\\
CIL\_30\_10\_100-mem100-resnet34-SGD-pretrained-drop0 &ResNet-34&yes& \textbf{60.37} & \textbf{93.19}\\
CIL\_30\_10\_100-mem100-resnet50-SGD-nopretrained-drop0 &ResNet-50&no& 50.43 & 86.28\\
CIL\_30\_10\_100-mem100-resnet50-SGD-pretrained-drop0 &ResNet-50&yes& 58.54 & 92.59\\
        \hline        
    \end{tabular}}
    \caption{Top-1 and Top-5 accuracy of the models at task 7.}
    \label{table:exp1}
\end{table}

\subsubsection{Regularization and data augmentation}
Following the analysis discussed above, it is necessary to regularize the model.
To do so, the next experiments are performed using the dropout layer (see \autoref{sec:methods-dropout}) and data augmentation (see \autoref{sec:methods-augment}). As said before, the backbone of each model is ResNet-34 and each one is pre-trained on ImageNet.

As expected, the performance at each task is higher than before, as shown in \autoref{fig:exp2}. Analyzing \autoref{fig:exp2-train_val}, the accuracy on the train and validation set at task 7 (the same as before), the training accuracy is lower than models without regularization, but the validation accuracy is higher. This is a sign that the regularization works as intended.

As shown in \autoref{table:exp2}, the top-1 accuracy of the best regularized model which adopting data augmentation is 7\% higher than that without regularization and data augmentation.

\begin{figure}[H]
	\centering
	\subfloat[\centering Top-1 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp2-top1.png} }}%
	\subfloat[\centering Top-5 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp2-top5.png} }}%
	\caption{Top-1 and Top-5 accuracy of the regularized models using data augmentation.}%
	\label{fig:exp2}%
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[\centering Accuracy on the training set]{{\includegraphics[width=0.49\textwidth]{images/exp/exp2-train.png} }}%
	\subfloat[\centering Accuracy on the validation set]{{\includegraphics[width=0.49\textwidth]{images/exp/exp2-val.png} }}%
	\caption{Regularized models with data augmentation: comparison of the accuracy at each training epoch at task 7 between training and validation.}%
	\label{fig:exp2-train_val}%
\end{figure}

\begin{table}[H]
    \centering
    \centerline{
    \begin{tabular}{c|c|c|c|c}
        \hline
        \textbf{Name} &
        \textbf{Data} &
        \textbf{Dropout} &
        \textbf{Top-1} & 
        \textbf{Top-5} \\
        &
        \textbf{augm.} &
        \textbf{rate} &
        \textbf{acc. (\%)} & 
        \textbf{acc. (\%)} \\
        \hline
        \hline
CIL\_30\_10\_100-mem100-resnet34-SGD-nopretrained-drop0 &no&0.0& 52.97 & 87.02\\
CIL\_30\_10\_100-mem100-resnet34-SGD-pretrained-drop0 &no&0.0& 60.37 & 93.19\\
\hline
CIL\_30\_10\_100-mem100-resnet34-SGD-pretrained-drop0.1-augmented&yes&0.1&	67.17&\textbf{98.15}\\
CIL\_30\_10\_100-mem100-resnet34-SGD-pretrained-drop0.3-augmented&yes&0.3&\textbf{67.28}&	97.3\\
CIL\_30\_10\_100-mem100-resnet34-SGD-pretrained-drop0.5-augmented&yes&0.5&65.57&	96.24\\
        \hline        
    \end{tabular}}
    \caption{Regularized models with data augmentation. Top-1 and Top-5 accuracy at task 7.}
    \label{table:exp2}
\end{table}

\subsubsection{Top-100 Classes}
In order to assess the extent to which those classes with few examples contribute to performance deterioration, the next experiments are carried out by considering only the top-100 classes, i.e. those with the largest number of examples (see the left-hand side of the distribution in \autoref{fig:logodet-dist}). This is useful to be aware of how much the model is limited in performance by the dataset.

Considering the regularized models and data augmentation, models trained on 100 randomly sampled classes and those trained on the top-100 classes are compared in \autoref{fig:exp3} and \autoref{table:exp3}. As we can see, even if the top-5 accuracy does not change between the two cases, considering the top-1 accuracy there is a drastic improvement in performance. This is a clear sign that classes with few examples deteriorate performance a lot.

\begin{figure}[H]
	\centering
	\subfloat[\centering Top-1 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp3-top1.png} }}%
	\subfloat[\centering Top-5 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp3-top5.png} }}%
	\caption{Comparison of models trained on 100 randomly sampled classed and top-100 classes.}%
	\label{fig:exp3}%
\end{figure}

\begin{table}[H]
    \centering
    \centerline{
    \begin{tabular}{c|c|c|c|c}
        \hline
        \textbf{Name} &
        \textbf{Dropout} &
        \textbf{Sampling} &
        \textbf{Top-1} & 
        \textbf{Top-5} \\
        &
        \textbf{rate} &
        \textbf{method} &
        \textbf{acc. (\%)} & 
        \textbf{acc. (\%)} \\
        \hline
        \hline
CIL\_30\_10\_100-mem100-[...]-drop0.1-augmented&0.1&random&	67.17&98.15\\
CIL\_30\_10\_100-mem100-[...]-drop0.3-augmented&0.3&random&67.28&	97.3\\
CIL\_30\_10\_100-mem100-[...]-drop0.5-augmented&0.5&random&65.57&	96.24\\
\hline
CIL\_30\_10\_100-mem100-[...]-drop0.1-augmented-onlytop&0.1&top-100&\textbf{89.1}&\textbf{96.59}\\
CIL\_30\_10\_100-mem100-[...]-drop0.3-augmented-onlytop&0.3&top-100&88.14&96.39\\
CIL\_30\_10\_100-mem100-[...]-drop0.5-augmented-onlytop&0.5&top-100&87.6&95.92\\
        \hline
    \end{tabular}}
    \caption{Comparison of models trained on 100 randomly sampled classed and top-100 classes. Top-1 and Top-5 accuracy at task 7.}
    \label{table:exp3}
\end{table}

\subsubsection{Introduction of Adam optimizer}
The next experiments aim to compare the SGD (see \autoref{sec:sgd_opt}) and Adam (see \autoref{sec:adam_opt}) optimizers. Considering the regularized models, data augmentation and the top-100 classes, the result of this comparison is shown in \autoref{fig:exp4} and \autoref{table:exp4}.

\begin{figure}[H]
	\centering
	\subfloat[\centering Top-1 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp4-top1.png} }}%
	\subfloat[\centering Top-5 accuracy]{{\includegraphics[width=0.50\textwidth]{images/exp/exp4-top5.png} }}%
	\caption{Comparison of models trained using SGD and Adam.}%
	\label{fig:exp4}%
\end{figure}

\begin{table}[H]
    \centering
    \centerline{
    \begin{tabular}{c|c|c|c|c}
        \hline
        \textbf{Name} &
        \textbf{Dropout} &
        \textbf{Optimizer} &
        \textbf{Top-1} & 
        \textbf{Top-5} \\
        &
        \textbf{rate} &
        \textbf{method} &
        \textbf{acc. (\%)} & 
        \textbf{acc. (\%)} \\
        \hline
        \hline
CIL\_30\_10\_100-[...]-SGD-pretrained-drop0.1-augmented-onlytop&0.1&SGD&89.1&96.59\\
CIL\_30\_10\_100-[...]-SGD-pretrained-drop0.3-augmented-onlytop&0.3&SGD&88.14&96.39\\
CIL\_30\_10\_100-[...]-SGD-pretrained-drop0.5-augmented-onlytop&0.5&SGD&87.6&95.92\\
\hline
CIL\_30\_10\_100-[...]-adam-pretrained-drop0.1-augmented-onlytop&0.1&Adam&92.15&97.92\\
CIL\_30\_10\_100-[...]-adam-pretrained-drop0.3-augmented-onlytop&0.3&Adam&91.55&97.75\\
CIL\_30\_10\_100-[...]-adam-pretrained-drop0.5-augmented-onlytop&0.5&Adam&\textbf{95.04}&\textbf{98.4}\\
        \hline
    \end{tabular}}
    \caption{Comparison of models trained using SGD and Adam. Top-1 and Top-5 accuracy at task 7.}
    \label{table:exp4}
\end{table}

Faster convergence and higher performance: From the training history we can see optimizer Adam migliora molto. \autoref{fig:exp4-train_val}.\todo{Spiegare}

\begin{figure}[H]
	\centering
	\subfloat[\centering Accuracy on the training set]{{\includegraphics[width=0.49\textwidth]{images/exp/exp4-train.png} }}%
	\subfloat[\centering Accuracy on the validation set]{{\includegraphics[width=0.49\textwidth]{images/exp/exp4-val.png} }}%
	\caption{Models trained using SGD and Adam: comparison of the accuracy at each training epoch at task 7 between the validation and training set.}%
	\label{fig:exp4-train_val}%
\end{figure}

\subsubsection{Pruning}
Ora\todo{Spiegare}

\subsection{2993 Classes}
\label{sec:whole_dataset_clf}
\section{Logo detector}
\label{sec:exp-det}
\subsection{Metrics}
\subsection{100 Classes}
\subsection{2993 Classes}
\subsubsection{Ablation study}
WA non utilizzato ma non peggiora molto le cose, probabilemnte perchè ci sono pochi esempi quindi qeusto non affect molto.

\section{Knowledge Distillation}
\label{sec:exp-kd}
\section{End to end classification}
\label{sec:exp-end2end}

% 700x400 vs 600x300 (small)